{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data from Wikipedia\n",
    "\n",
    "This notebook covers the basics of extracting text data from wikipedia as a base for NLP tasks.\n",
    "\n",
    "Based on this [KDNuggets Tutorial Post](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html)\n",
    "\n",
    "Other Resources:\n",
    "- Wikipedia [dumpfile collection](https://dumps.wikimedia.org/enwiki/latest/)\n",
    "- Documentation on [gensim](https://radimrehurek.com/gensim/corpora/wikicorpus.html)\n",
    "\n",
    "For requirements, run: `pip3 install -r requirements.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dependencies we need for processing wikipedia data\n",
    "import multiprocessing\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "wiki_infile = 'data/enwiki-latest-pages-articles1.xml-p10p30302.bz2'\n",
    "wiki_outfile = 'data/wiki.en.txt'\n",
    "\n",
    "def store_wiki_dump(infile, outfile):\n",
    "    '''\n",
    "    Accepts a wikipedia dump file as 'wiki_infile', processes the wiki data,\n",
    "    and stores raw text to 'wiki_outfile'.\n",
    "    '''\n",
    "    \n",
    "    # Using gensim.corpora.wikicorpus, we can process a wiki dump file\n",
    "    wiki = WikiCorpus(infile, lemmatize=False, dictionary={})\n",
    "\n",
    "    # Save the processed XML wiki dump as raw text in file\n",
    "    # Depending on the size of the dump (usually quite large), this could take hours\n",
    "    with open(wiki_outfile, 'w') as outfile:\n",
    "        i = 0\n",
    "        for text in wiki.get_texts():\n",
    "            outfile.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "            i = i + 1\n",
    "            if (i % 2000 == 0):\n",
    "                print('Processed ' + str(i) + ' articles')\n",
    "        print('Done processing wiki data.')\n",
    "        \n",
    "store_wiki_dump(wiki_infile, wiki_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "def check_corpus(input_file):\n",
    "    \"\"\"\n",
    "    Reads some lines of corpus from text file\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        for lines in range(50):\n",
    "            print(input_file.readline())\n",
    "        user_input = input('>>> Continue to next line of text? [y|N] <<< ')\n",
    "        if user_input == 'N':\n",
    "            break\n",
    "            \n",
    "def load_corpus(input_file):\n",
    "    \"\"\"\n",
    "    Loads corpus from text file\n",
    "    \"\"\"\n",
    "\n",
    "    print('Loading corpus...')\n",
    "    time1 = time.time()\n",
    "    corpus = input_file.read()\n",
    "    time2 = time.time()\n",
    "    total_time = time2-time1\n",
    "    print('It took %0.3f seconds to load corpus' %total_time)\n",
    "    return corpus\n",
    "\n",
    "def run():\n",
    "    if len(sys.argv) != 2:\n",
    "        print('Usage: python check_wiki_corpus.py <corpus_file>')\n",
    "        sys.exit(1)\n",
    "\n",
    "    corpus_file = open(sys.argv[1],'r')\n",
    "    check_corpus(corpus_file)\n",
    "    corpus = load_corpus(corpus_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
